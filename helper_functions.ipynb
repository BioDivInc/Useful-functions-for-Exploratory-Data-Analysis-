{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b76438c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. helper function: Print out basic information about the loaded dataset\n",
    "def print_bascis(dataset):\n",
    "    \"\"\"\n",
    "    Prints out basic information about a loaded dataset:\\n\n",
    "    Sample of 10 rows,\\n\n",
    "    Dataset dimensions,\\n\n",
    "    Dataset dtypes,\\n\n",
    "    Dataset dtype summary,\\n\n",
    "    NAs check,\\n\n",
    "    Description of numeric features.\n",
    "    \"\"\"\n",
    "    # install and import 'tabulate' for a cleaner output of dataset as well as pandas \n",
    "    try:\n",
    "        from tabulate import tabulate\n",
    "        print(\"Imported tabulate\")\n",
    "        import pandas as pd\n",
    "        print(\"Imported pandas.\")\n",
    "        from IPython.display import display\n",
    "        print(\"Imported display.\")\n",
    "    except:\n",
    "        %pip install tabulate\n",
    "        from tabulate import tabulate\n",
    "        print(\"Installed and imported tabulate.\")\n",
    "        %pip install pandas\n",
    "        import pandas as pd\n",
    "        print(\"Installed and imported pandas.\")\n",
    "\n",
    "    # set options to display dataset\n",
    "    pd.set_option('display.width', 1000) # control width\n",
    "    pd.set_option('display.max_columns', None)  # show all columns\n",
    "    pd.set_option('display.max_colwidth', None)  # don't truncate column content\n",
    "\n",
    "    # print a sample of dataset\n",
    "    print(f\"\\nSample of 10 rows of the dataset:\")\n",
    "    print(tabulate(dataset.sample(10), headers='keys', tablefmt='rounded_grid'))\n",
    "\n",
    "    # check the dimensions of the dataset\n",
    "    print(f\"Dataset dimensions: {dataset.shape[0]} rows and {dataset.shape[1]} columns.\")\n",
    "\n",
    "    # print out the dtype for each column found\n",
    "    dtype_table = [(col, dtype) for col, dtype in dataset.dtypes.items()]\n",
    "    print(\"\\nColumn data types:\")\n",
    "    print(tabulate(dtype_table, headers=[\"column name\", \"data type\"], tablefmt=\"rounded_grid\"))\n",
    "\n",
    "    # print summary table for dtypes\n",
    "    df_dtype = pd.DataFrame(dtype_table, columns=['column name', 'data type'])\n",
    "    print(\"\\nSummary table dtypes:\")\n",
    "    print(tabulate(\n",
    "        df_dtype['data type'].value_counts().reset_index().values.tolist(),\n",
    "        headers=['data type', 'count'],\n",
    "        tablefmt='rounded_grid'\n",
    "    ))\n",
    "\n",
    "    # check for NAs\n",
    "    print(\"\\nLooking for NAs...\", end=\"\")\n",
    "    if dataset.isna().sum().sum() > 0:\n",
    "        print(\"Found NAs:\")\n",
    "        df_na = dataset.isna().sum()\n",
    "        print(tabulate(df_na.reset_index().values.tolist(), headers=['column name', 'count'], tablefmt='rounded_grid'))\n",
    "    else:\n",
    "        print(\"No NAs found.\")\n",
    "    \n",
    "    # print out a description of numeric values (mean, max, ...)\n",
    "    dataset = dataset.select_dtypes(include=['int', 'float'])\n",
    "    print(\"\\nSummary of numerical features:\")\n",
    "    display(dataset.describe().T.style.format(\"{:.2f}\").background_gradient(cmap='Blues')) # description of numerical features with transposed rows, backed with a blue gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc36758",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. helper function: Visual exploration of categorical features\n",
    "def EDA_categorical(dataset, ncols:int, batch_size:int, dpi:int, key:str):\n",
    "    \"\"\"\n",
    "    Visualizes the distribution of categorical columns in a loaded dataset, \\n\n",
    "    dataset = any dataframe, \\n\n",
    "    ncols = any number of columns you want the gridsize to be, \\n\n",
    "    batch_size = any number of figures you want to output at once, \\n\n",
    "    dpi = dots per inch resolution for plotting, \\n\n",
    "    key = any categorical feature you want to classify your data by.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        import seaborn as sns\n",
    "        print(\"Imported seaborn.\")\n",
    "        import matplotlib.pyplot as plt\n",
    "        print(\"Imported matplotlib.\")\n",
    "        import math\n",
    "        print(\"Imported math.\")\n",
    "    except:\n",
    "        %pip install seaborn \n",
    "        import seaborn as sns\n",
    "        print(\"Installed and imported seaborn.\")\n",
    "        %pip install matplotlib\n",
    "        import matplotlib.pyplot as plt\n",
    "        print(\"Installed and imported matplotlib.\")\n",
    "        %pip install math\n",
    "        import math\n",
    "        print(\"Installed and imported math.\")\n",
    "\n",
    "    # create lists with total number of numeric columns (int) and column names \n",
    "    n_features_col=dataset.select_dtypes(include=['object']).columns # name of columns with most common numeric dtypes\n",
    "    n_features_int=len(n_features_col) # number of columns with most common numeric dtypes\n",
    "\n",
    "    # create chunks for the plots\n",
    "    for chunk_start in range(0, n_features_int, batch_size): # starts at 0, goes up to number of numerical features and increments by set batch_size\n",
    "        chunk = n_features_col[chunk_start:chunk_start + batch_size] # slices the list into chunks defined by batch_size\n",
    "\n",
    "        # specify figsize and dpi\n",
    "        plt.figure(figsize=(10, 5), dpi=dpi)\n",
    "\n",
    "        # loop through chunks\n",
    "        for i, feature in enumerate(chunk, 1):  # Start from 1 for subplot index\n",
    "            \n",
    "            # calculates how many full rows are needed if you have ncols columns\n",
    "            if n_features_int % 2 == 0: # if even:\n",
    "                nrow_ncol=int(math.ceil(n_features_int) / ncols) # grid will fit nicely\n",
    "            else: #if odd:\n",
    "                nrow_ncol=int(math.ceil(n_features_int) / ncols)+1 # + 1 if odd\n",
    "\n",
    "            # define subplot arrangement\n",
    "            plt.subplot(nrow_ncol, ncols, i)\n",
    "\n",
    "            # histogram\n",
    "            if key is not None: # if classification (e.g., by 'species', 'id', ...) is given\n",
    "                sns.histplot(data=dataset, x=feature, kde=False, hue=key, bins=50, stat='count', multiple='stack')\n",
    "            else:\n",
    "                sns.histplot(data=dataset, x=feature, kde=False, bins=50, stat='count', multiple='stack')\n",
    "            plt.title(f'Distribution of {feature.capitalize()}')\n",
    "            plt.xlabel(feature.capitalize())\n",
    "            plt.ylabel('Count')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f64160",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. helper function: Visual exploration of numerical features\n",
    "def EDA_numerical(dataset, ncols:int, batch_size:int, dpi:int, key:str, type:str):\n",
    "    \"\"\"\n",
    "    Visualizes the distribution of numerical columns in a loaded dataset. \\n\n",
    "    dataset = any dataframe, \\n\n",
    "    ncols = any number of columns you want the gridsize to be, \\n\n",
    "    batch_size = any number of figures you want to output at once, \\n\n",
    "    dpi = dots per inch resolution for plotting, \\n\n",
    "    key = any categorical feature you want to classify your data by, \\n\n",
    "    type = type of plot (i.e., 'hist', 'box', 'bar', 'violin') you want to output.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        import seaborn as sns\n",
    "        print(\"Imported seaborn.\")\n",
    "        import matplotlib.pyplot as plt\n",
    "        print(\"Imported matplotlib.\")\n",
    "        import math\n",
    "        print(\"Imported math.\\n\")\n",
    "    except:\n",
    "        %pip install seaborn \n",
    "        import seaborn as sns\n",
    "        print(\"Installed and imported seaborn.\")\n",
    "        %pip install matplotlib\n",
    "        import matplotlib.pyplot as plt\n",
    "        print(\"Installed and imported matplotlib.\")\n",
    "        %pip install math\n",
    "        import math\n",
    "        print(\"Installed and imported math.\\n\")\n",
    "\n",
    "    # create list with names of numeric columns and the total number of numeric features \n",
    "    n_features_col=dataset.select_dtypes(include=['int', 'float']).columns # name of columns with most common numeric dtypes\n",
    "    n_features_int=len(n_features_col) # number of columns with most common numeric dtypes\n",
    "\n",
    "    # create chunks/batches for the plots\n",
    "    for chunk_start in range(0, n_features_int, batch_size): # starts at 0, goes up to number of numerical features and increments by set batch_size\n",
    "        chunk = n_features_col[chunk_start:chunk_start + batch_size] # slices the list into chunks defined by batch_size\n",
    "\n",
    "        # specify figsize and dpi\n",
    "        plt.figure(figsize=(10, 16), dpi=dpi)\n",
    "\n",
    "        # loop through chunks\n",
    "        for i, feature in enumerate(chunk, 1):  # Start from 1 for subplot index\n",
    "\n",
    "            # calculates how many full rows are needed if you have ncols columns\n",
    "            if n_features_int % 2 == 0: # if even:\n",
    "                nrow_ncol=int(math.ceil(n_features_int) / ncols) # grid will fit nicely\n",
    "            else: #if odd:\n",
    "                nrow_ncol=int(math.ceil(n_features_int) / ncols)+1 # + 1 if odd\n",
    "            \n",
    "            # define subplot arrangement\n",
    "            plt.subplot(nrow_ncol, ncols, i)\n",
    "\n",
    "            # entered type will device which plot to output\n",
    "            # histogram\n",
    "            if type == 'hist':\n",
    "                if key is not None: \n",
    "                    sns.histplot(data=dataset, x=feature, kde=True, hue=key, bins=30)\n",
    "                else: \n",
    "                    sns.histplot(dataset[feature], kde=True, bins=30)\n",
    "                plt.title(f'Distribution of {feature.capitalize()}')\n",
    "                plt.xlabel(feature.capitalize())\n",
    "                plt.ylabel('Count')\n",
    "\n",
    "            # boxplot\n",
    "            elif type == 'box':\n",
    "                if key is not None:\n",
    "                    sns.boxplot(data=dataset, x=feature, y=key, hue=key)\n",
    "                    plt.ylabel(key.capitalize())\n",
    "                else: \n",
    "                    sns.boxplot(data=dataset, x=feature)\n",
    "                    plt.yticks([])\n",
    "                plt.title(f'Distribution of {feature.capitalize()}')\n",
    "                plt.xlabel(feature.capitalize())\n",
    "\n",
    "            # barplot\n",
    "            elif type == 'bar':\n",
    "                if key is not None: \n",
    "                    sns.barplot(data=dataset, x=feature, y=key, hue=key)\n",
    "                    plt.ylabel(key.capitalize())\n",
    "                else: \n",
    "                    sns.barplot(data=dataset, x=feature)\n",
    "                    plt.yticks([])\n",
    "                plt.title(f'Distribution of {feature.capitalize()}')\n",
    "                plt.xlabel(feature.capitalize())\n",
    "\n",
    "            # violin plot\n",
    "            elif type == 'violin':\n",
    "                if key is not None: \n",
    "                    sns.violinplot(data=dataset, x=feature, y=key, hue=key)\n",
    "                    plt.ylabel(key.capitalize())\n",
    "                else: \n",
    "                    sns.violinplot(data=dataset, x=feature)\n",
    "                    plt.yticks([])\n",
    "                plt.title(f'Distribution of {feature.capitalize()}')\n",
    "                plt.xlabel(feature.capitalize())\n",
    "\n",
    "            # type is not supported\n",
    "            else:\n",
    "                print(f\"Type is not supported.\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3872b4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. helper function: Summary of statistical tests to find a fitting correlation coefficient\n",
    "def correlation_summary(dataset, ncols:int, batch_size_per_feature:int, dpi:int, key:str):\n",
    "    \"\"\"\n",
    "    Performs (statistical) tests to determine the right correlation coefficient and visualizes the relationship (i.e., correlation) of numercial features in a loaded dataset.\\n\n",
    "    dataset = any dataframe, \\n\n",
    "    ncols = any number of columns you want the gridsize to be, \\n\n",
    "    batch_size_per_feature = any number of figures you want to output at once (per feature), \\n\n",
    "    dpi = dots per inch resolution for plotting, \\n\n",
    "    key = any categorical feature you want to classify your data by.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        import pandas as pd\n",
    "        print(\"Imported pandas\")\n",
    "        import matplotlib.pyplot as plt\n",
    "        print(\"Imported matplotlib.\")\n",
    "        import seaborn as sns\n",
    "        print(\"Imported seaborn.\")\n",
    "        import scipy\n",
    "        from scipy import stats\n",
    "        print(\"Imported scipy and stats.\")\n",
    "        import math\n",
    "        print(\"Imported math.\")\n",
    "        import numpy as np\n",
    "        from numpy import trapezoid\n",
    "        print(\"Imported numpy and trapezoid.\")  \n",
    "        import pylab\n",
    "        print(\"Imported pylab.\")\n",
    "        import tabulate\n",
    "        from tabulate import tabulate\n",
    "        print(\"Imported tabulate\")\n",
    "    except:\n",
    "        %pip install pandas \n",
    "        import pandas as pd\n",
    "        print(\"Installed and imported pandas.\")\n",
    "        %pip install matplotlib\n",
    "        import matplotlib.pyplot as plt\n",
    "        print(\"Installed and imported matplotlib.\")\n",
    "        %pip install seaborn\n",
    "        import seaborn as sns\n",
    "        print(\"Installed and imported seaborn.\")\n",
    "        %pip install scipy\n",
    "        import scipy\n",
    "        from scipy import stats\n",
    "        print(\"Installed, imported scipy and stats.\")\n",
    "        %pip install math\n",
    "        import math\n",
    "        print(\"Installed and imported math.\")\n",
    "        %pip install numpy\n",
    "        import numpy as np\n",
    "        from numpy import trapezoid\n",
    "        print(\"Installed, imported numpy and trapezoid.\")\n",
    "        %pip install pylab\n",
    "        import pylab\n",
    "        print(\"Installed and imported pylab.\")\n",
    "        %pip install tabulate\n",
    "        import tabulate\n",
    "        from tabulate import tabulate\n",
    "        print(\"Installed and imported tabulate\")\n",
    "    \n",
    "    # summary of functions used to clean up the appearance and simplify 'correlation summary':\n",
    "\n",
    "    # define function for a normal distribution layover\n",
    "    def add_fit_to_histplot(a, fit=stats.norm, ax=None): # credits:https://stackoverflow.com/questions/64621456/plotting-a-gaussian-fit-to-a-histogram-in-displot-or-histplot\n",
    "\n",
    "        if ax is None:\n",
    "            ax = plt.gca()\n",
    "\n",
    "        # compute bandwidth\n",
    "        bw = len(a)**(-1/5) * a.std(ddof=1)\n",
    "        # initialize PDF support\n",
    "        x = np.linspace(a.min()-bw*3, a.max()+bw*3, 200)\n",
    "        # compute PDF parameters\n",
    "        params = fit.fit(a)\n",
    "        # compute PDF values\n",
    "        y = fit.pdf(x, *params)\n",
    "        # plot the fitted continuous distribution\n",
    "        ax.plot(x, y, color=\"#c44e52\", linestyle='dashed')\n",
    "        return ax\n",
    "    \n",
    "    # prepare dataset and perform statistics on it\n",
    "    def perform_statistics(data):\n",
    "        \n",
    "        # only fetch numerical features from dataset\n",
    "        dataset_num = data.select_dtypes(include=['int', 'float'])\n",
    "\n",
    "        # create empty list for the results to be saved in\n",
    "        results=[]\n",
    "\n",
    "        # 1. loop through subsetted dataset with numerical data\n",
    "        for feature in dataset_num:\n",
    "\n",
    "            # test for normality\n",
    "            stat_normal, p_value_normal = stats.normaltest(dataset_num[feature])\n",
    "            stat_shapiro, p_value_shapiro = stats.shapiro(dataset_num[feature])\n",
    "            \n",
    "            # add pp-plot and calc the auc, compare with reference of linear reference line\n",
    "            df_pp= dataset_num[feature]\n",
    "            (prob_x, sample_y), (slope, intercept, r) = stats.probplot(df_pp, dist=\"norm\")\n",
    "            auc_dataset = trapezoid(sample_y, prob_x)\n",
    "\n",
    "            # Predicted y values from the pp line\n",
    "            ref_y = slope * prob_x + intercept\n",
    "            (prob_x_ref, sample_y_ref), (slope_ref, intercept_ref, r_ref) = stats.probplot(ref_y, dist=\"norm\")\n",
    "            auc_ref = trapezoid(sample_y_ref, prob_x_ref)\n",
    "            \n",
    "            # difference between auc dataset and auc reference (%)\n",
    "            auc_diff = round(((auc_dataset-auc_ref)/auc_ref)*100, 2)\n",
    "            if auc_diff < 0:\n",
    "                auc_diff = auc_diff*-1\n",
    "\n",
    "            # Mean Squared Deviation (MSD)\n",
    "            msd_absolute = np.mean((sample_y - ref_y) ** 2) # mean(sqrt(observed data-expected data))\n",
    "\n",
    "            # check for length of features\n",
    "            feature_n = dataset_num[feature].notna().size\n",
    "\n",
    "            # check for duplicate values, fetch counts and calc the difference to feature_n (%)\n",
    "            duplicates = dataset_num[feature].duplicated().value_counts() # outputs True and False with respective count values\n",
    "            raw_duplicate_count = duplicates.get(True, 0) # only gets true values, if not true -> returns 0 \n",
    "            raw_duplicate_percent = round((raw_duplicate_count/feature_n)*100, 2) # percentage of duplicates for each feature\n",
    "\n",
    "            results.append({    \n",
    "                \"feature\": feature,\n",
    "                \"feature: n\": feature_n,\n",
    "                \"duplicates absolute\": raw_duplicate_count,\n",
    "                \"duplicates %\": raw_duplicate_percent,\n",
    "                \"statistic normal\": stat_normal,\n",
    "                \"statistic shapiro\": stat_shapiro,\n",
    "                \"p-value normal\": p_value_normal,\n",
    "                \"p-value shapiro\": p_value_shapiro,\n",
    "                \"auc data\": auc_dataset,\n",
    "                \"auc reference\": auc_ref,\n",
    "                \"auc diff\": auc_diff,\n",
    "                \"msd\": msd_absolute\n",
    "            })\n",
    "\n",
    "        # print out df with saved information\n",
    "        results_df = pd.DataFrame(results)\n",
    "        print(f\"\\nSummary of statistical tests to determine best fitting correlation metric.\")\n",
    "        headers=['Feature: name', 'Feature: n', 'Duplicates: count', 'Duplicates: %', 'Normaltest: statistic', 'Shapiro Wilk: statistic', 'Normaltest: p-value', 'Shapiro Wilk: p-value', 'AUC: loaded dataset', 'AUC: reference line', 'AUC: absolute difference (%)', 'Mean Squared Deviation (MSD)']\n",
    "        print(tabulate(results_df.values.tolist(),headers=headers, tablefmt='rounded_grid'))\n",
    "        \n",
    "        # 2. assign correlation coefficient to features\n",
    "        for index, row in results_df.iterrows(): # iterate through rows of results_df to find features which meet defined conditions\n",
    "\n",
    "            score=0\n",
    "\n",
    "            # define conditions for suggested categorization of correlation coefficients\n",
    "            #condition_tests = row['p-value shapiro'] > 0.05 and row['p-value normal'] > 0.05 # if greater than 0.05, considered normally distributed\n",
    "            condition_auc = row['auc diff'] <= 3.5  # set an MSD of 0.1 as threshold for now\n",
    "            condition_msd = row['msd'] <= 0.5\n",
    "            condition_n = row['feature: n'] > 30 # if length of feature is greater than 30\n",
    "            condition_ties = row['duplicates %'] < 20 # if less than 20 % of duplicates/tied ranks\n",
    "            condition_skew_kurtosis = row['statistic normal'] < 55 # if kurtosis, skew score is smaller than 50\n",
    "\n",
    "            # add a weight to condtions\n",
    "            if condition_msd:\n",
    "                score += 2\n",
    "            if condition_auc:\n",
    "                score += 2\n",
    "            if condition_skew_kurtosis:\n",
    "                score += 1\n",
    "            #if condition_tests:\n",
    "                #score += 0.5 \n",
    "            if condition_n:\n",
    "                score += 0.25\n",
    "            if condition_ties:\n",
    "                score += 0.25\n",
    "            \n",
    "            # append to lists\n",
    "            if key is not None:\n",
    "                if score >= 5:\n",
    "                    pearson.append({\"feature\": row['feature'], \"subset\": value})\n",
    "                elif score >= 0.5:\n",
    "                    spearman.append({\"feature\": row['feature'], \"subset\": value})\n",
    "                else:\n",
    "                    kendall.append({\"feature\": row['feature'], \"subset\": value})\n",
    "            else:\n",
    "                if score >= 5:\n",
    "                    pearson.append({\"feature\": row['feature']})\n",
    "                elif score >= 0.5:\n",
    "                    spearman.append({\"feature\": row['feature']})\n",
    "                else:\n",
    "                    kendall.append({\"feature\": row['feature']})\n",
    "\n",
    "    # prepare dataset and plot histogram + pp-plot\n",
    "    def plot_hist_pp(data):\n",
    "\n",
    "        # prepare subset for plotting\n",
    "        n_features_col = data.select_dtypes(include=['int', 'float']).columns # selects only numeric features\n",
    "        n_features_int = len(n_features_col) # length of numeric features\n",
    "\n",
    "        # separat subsetted data into chunks of batch_size_per_feature  \n",
    "        for chunk_start in range(0, n_features_int, batch_size_per_feature): # starts at 0, goes up to number of numerical features and increments by set batch_size\n",
    "            chunk = n_features_col[chunk_start:chunk_start + batch_size_per_feature] # slices the list into chunks defined by batch_size\n",
    "            n_features = len(chunk)\n",
    "            \n",
    "            # set some parameters\n",
    "            n_subplots = n_features * 2  # hist + pp-plot for each feature\n",
    "            nrows = math.ceil(n_subplots / ncols)\n",
    "            plt.figure(figsize=(10, 6), dpi=dpi)\n",
    "\n",
    "            # plot histogram and pp-plot per feature\n",
    "            for i, feature in enumerate(chunk):\n",
    "\n",
    "                # histogram\n",
    "                plt.subplot(nrows, ncols, 2 * i + 1)\n",
    "                sns.histplot(data[feature], kde=True, bins=50, stat='density')\n",
    "                add_fit_to_histplot(data[feature], fit=stats.norm)\n",
    "                plt.title(f'Distribution of {feature.capitalize()}')\n",
    "                plt.xlabel(feature.capitalize())\n",
    "                plt.ylabel('Density')\n",
    "\n",
    "                # pp-plot\n",
    "                plt.subplot(nrows, ncols, 2 * i + 2)\n",
    "                stats.probplot(data[feature], dist=\"norm\", plot=pylab)\n",
    "                plt.title(f'P-P plot of {feature.capitalize()}')\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "\n",
    "    # start with pipeline\n",
    "\n",
    "    # if key is provided:\n",
    "    if key is not None:\n",
    "        \n",
    "        # 1. create empty lists for the results to be saved in\n",
    "        pearson = []\n",
    "        spearman = []\n",
    "        kendall = []\n",
    "\n",
    "        # 2. get unique names of provided key\n",
    "        class_names = dataset[key].unique()\n",
    "\n",
    "        # 3. subset data based on provided key\n",
    "        for value in class_names:\n",
    "            subset = dataset[dataset[key] == value]\n",
    "            print(f\"\\nPerformed subsetting for '{value}' of provided key '{key}'.\")\n",
    "        \n",
    "            # 4. prepare datset and perform statistics on it + assign features to correlation coefficient based on met conditions\n",
    "            perform_statistics(data=subset)\n",
    "\n",
    "            # 5. prepare dataset and plot histogram+pp-plot with reference curve+line to display normal distribution\n",
    "            plot_hist_pp(data=subset)\n",
    "            \n",
    "        # 6. convert filled lists to df, ignore empty lists and print out dfs\n",
    "        if len(pearson) == 0 and len(spearman) == 0 and len(kendall) == 0:\n",
    "            print(\"Lists are empty. No classification could be performed.\")\n",
    "        if len(pearson) != 0:\n",
    "            pearson_df = pd.DataFrame(pearson)\n",
    "            pearson = pd.to_pickle(pearson_df, 'pearson.pkl')\n",
    "            print(\"\\nSuggested correlation coefficient: pearson's r.\")\n",
    "            print(tabulate(pearson_df.values.tolist(),headers=['Feature: name', 'Subset: name'], tablefmt='rounded_grid'))\n",
    "        if len(spearman) !=0:\n",
    "            spearman_df = pd.DataFrame(spearman)\n",
    "            spearman = pd.to_pickle(spearman_df, 'spearman.pkl')\n",
    "            print(\"\\nSuggested correlation coefficient: spearman's ρ.\")\n",
    "            print(tabulate(spearman_df.values.tolist(),headers=['Feature: name', 'Subset: name'], tablefmt='rounded_grid'))\n",
    "        if len(kendall) != 0:\n",
    "            kendall_df = pd.DataFrame(kendall)\n",
    "            kendall = pd.to_pickle(kendall_df, 'kendall.pkl')\n",
    "            print(\"\\nSuggested correlation coefficient: kendall's τ.\")\n",
    "            print(tabulate(kendall_df.values.tolist(),headers=['Feature: name', 'Subset: name'], tablefmt='rounded_grid'))\n",
    "\n",
    "\n",
    "    # if no key is provided  \n",
    "    else:\n",
    "\n",
    "        # 1. create empty lists for the results to be saved in\n",
    "        pearson = []\n",
    "        spearman = []\n",
    "        kendall = []\n",
    "\n",
    "        # 2. prepare datset and perform statistics on it + assign features to correlation coefficient based on met conditions\n",
    "        perform_statistics(data=dataset)\n",
    "\n",
    "        # 3. prepare dataset and plot histogram+pp-plot with reference curve+line to display normal distribution\n",
    "        plot_hist_pp(data=dataset)\n",
    "\n",
    "        # 4. convert filled lists to df, ignore empty lists and print out dfs\n",
    "        if len(pearson) == 0 and len(spearman) == 0 and len(kendall) == 0:\n",
    "            print(\"Lists are empty. No classification could be performed.\")\n",
    "        if len(pearson) != 0:\n",
    "            pearson_df = pd.DataFrame(pearson)\n",
    "            pearson = pd.to_pickle(pearson_df, 'pearson.pkl')\n",
    "            print(\"\\nSuggested correlation coefficient: pearson's r.\")\n",
    "            print(tabulate(pearson_df.values.tolist(),headers=['Feature: name'], tablefmt='rounded_grid'))\n",
    "        if len(spearman) !=0:\n",
    "            spearman_df = pd.DataFrame(spearman)\n",
    "            spearman = pd.to_pickle(spearman_df, 'spearman.pkl')\n",
    "            print(\"\\nSuggested correlation coefficient: spearman's ρ.\")\n",
    "            print(tabulate(spearman_df.values.tolist(),headers=['Feature: name'], tablefmt='rounded_grid'))\n",
    "        if len(kendall) != 0:\n",
    "            kendall_df = pd.DataFrame(kendall)\n",
    "            kendall = pd.to_pickle(kendall_df, 'kendall.pkl')\n",
    "            print(\"\\nSuggested correlation coefficient: kendall's τ.\")\n",
    "            print(tabulate(kendall_df.values.tolist(),headers=['Feature: name'], tablefmt='rounded_grid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460801c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. helper function: Processing statistical information and visualize relationships\n",
    "def correlation_visualization(dataset, corr:str, dpi:int, key:str): \n",
    "    \"\"\"\n",
    "    Visualizes the relationship (i.e., correlation) of numercial features in a loaded dataset.\\n\n",
    "    dataset = any dataframe, \\n\n",
    "    corr = correlation coefficient (i.e., 'pearson', 'spearman', 'kendall'); 'auto' = takes suggested classification by the function 'correlation_summary', \\n\n",
    "    dpi = dots per inch resolution for plotting, \\n\n",
    "    key = any categorical feature you want to classify your data by.\n",
    "    \"\"\"\n",
    "    try:   \n",
    "        import pandas as pd\n",
    "        print(\"Imported pandas\")\n",
    "        import seaborn as sns\n",
    "        print(\"Imported seaborn.\")\n",
    "        import matplotlib.pyplot as plt\n",
    "        print(\"Imported matplotlib.\")\n",
    "        from tabulate import tabulate\n",
    "        print(\"Imported tabulate.\")\n",
    "        from pathlib import Path\n",
    "        print(\"Imported Path.\")\n",
    "    except:\n",
    "        %pip install seaborn\n",
    "        import seaborn as sns\n",
    "        print(\"Installed and imported seaborn.\")\n",
    "        %pip install matplotlib\n",
    "        import matplotlib.pyplot as plt\n",
    "        print(\"Installed and imported matplotlib.\")\n",
    "        %pip install tabulate\n",
    "        from tabulate import tabulate\n",
    "        print(\"Installed and imported tabulate.\")\n",
    "        %pip install pathlib\n",
    "        from pathlib import Path\n",
    "        print(\"Installed and imported pathlib.\")\n",
    "\n",
    "    # set some visualization parameters\n",
    "    sns.set_theme(style=\"whitegrid\") # appearance\n",
    "    plt.figure(figsize=(10, 5), dpi=dpi) \n",
    "\n",
    "    # overview of functions used:\n",
    "\n",
    "    # set up 'corr' + respective suffix\n",
    "    def determine_suffix(coefficient:str):\n",
    "        if coefficient == 'pearson':\n",
    "            suffix = \"'s r\"\n",
    "        elif coefficient == 'spearman':\n",
    "            suffix = \"'s ρ\"\n",
    "        else:\n",
    "            suffix = \"'s τ\"\n",
    "        return coefficient+suffix\n",
    "    \n",
    "    # prepare dataset and plot correlation matrix:\n",
    "    def prepare_and_plot(data, coefficient:str):\n",
    "\n",
    "        # call function 'determine_suffix' to create the full name of the used correlation coefficient\n",
    "        corr_full_name = determine_suffix(path.stem if corr == 'auto' else corr)\n",
    "\n",
    "        # select only numerical columns\n",
    "        numeric_df = data.select_dtypes(include=['int', 'float'])\n",
    "\n",
    "        # Compute the correlation matrix using the specified method\n",
    "        corr_mat = numeric_df.corr(method=coefficient).stack().reset_index(name=coefficient)\n",
    "\n",
    "        # Create the plot\n",
    "        rel = sns.relplot(\n",
    "            data=corr_mat,\n",
    "            x=\"level_0\", y=\"level_1\", hue=coefficient, size=coefficient,\n",
    "            palette=\"vlag\", hue_norm=(-1, 1), edgecolor=\".5\",\n",
    "            height=10, sizes=(50, 300), size_norm=(-.2, .8)\n",
    "        )\n",
    "        rel.set(xlabel=\"\", ylabel=\"\", aspect=\"equal\")\n",
    "        rel.despine(left=True, bottom=True)\n",
    "\n",
    "        # add title if key is not None for more information about the used dataset \n",
    "        if key is not None:\n",
    "            plt.title(f\"{corr_full_name.capitalize()} applied on categorical feature '{key.capitalize()}' with subset of '{class_name.capitalize()}'\")\n",
    "\n",
    "        rel._legend.set_title(f\"{corr_full_name.capitalize()}\") # adjust legend title \n",
    "        plt.subplots_adjust(right=0.89) # leave some space for the legend title\n",
    "        rel.ax.margins(.02)\n",
    "        for label in rel.ax.get_xticklabels():\n",
    "            label.set_rotation(90)\n",
    "            label.set_horizontalalignment('right')\n",
    "        plt.show()\n",
    "        \n",
    "    # if no key is provided:\n",
    "    if key is None:\n",
    "\n",
    "        # if no correlation coefficient is entered, but auto categorization is desired\n",
    "        if corr == 'auto':\n",
    "            \n",
    "            # file names of pickled data\n",
    "            filenames = ['pearson.pkl', 'spearman.pkl', 'kendall.pkl']\n",
    "            # create empty list for actually existing files\n",
    "            existing_files = []\n",
    "\n",
    "            # 1. Check which files exist\n",
    "            for file_name in filenames:\n",
    "                path = Path(file_name)\n",
    "                if path.is_file():\n",
    "                    existing_files.append(path) # update list if existent\n",
    "                else: # print non-existent file names\n",
    "                    print(f\"\\n{file_name} not existent. Loading existent ones...\", end=\"\")\n",
    "\n",
    "            # Dictionary to hold loaded dataframes\n",
    "            dfs_dict = {}\n",
    "\n",
    "            # 2. Load existing .pkl files\n",
    "            for path in existing_files:\n",
    "                try:\n",
    "                    # load pickled files \n",
    "                    df = pd.read_pickle(path)\n",
    "                    # create df with correlation coefficient names and get rid of the suffix\n",
    "                    dfs_dict[path.stem] = df \n",
    "                    print(f\"\\nLoaded {path.name}:\")\n",
    "                    print(tabulate(df, tablefmt='rounded_grid'))\n",
    "\n",
    "                    # Extract values\n",
    "                    values = df['feature'].tolist()\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error loading {path.name}: {e}\")\n",
    "                \n",
    "                # 3. create subsets\n",
    "                try:\n",
    "                    subset = dataset[values]\n",
    "                except KeyError as e:\n",
    "                    print(f\"{path.stem} not found in dataset: {e}\")\n",
    "\n",
    "                # 4. prepare dataset and plot correlation matrix\n",
    "                prepare_and_plot(data=subset, coefficient=path.stem)\n",
    "\n",
    "        # if correlation coefficient ('pearson', 'spearman', 'kendall') is provided\n",
    "        else:\n",
    "\n",
    "            # prepare datasets and plot correlation matrix \n",
    "            prepare_and_plot(data=dataset, coefficient=corr)\n",
    "\n",
    "\n",
    "    # if key is provided:\n",
    "    else:\n",
    "\n",
    "        # if auto categorization of correlation coefficients:\n",
    "        if corr == 'auto':\n",
    "\n",
    "            # file names of pickled data\n",
    "            filenames = ['pearson.pkl', 'spearman.pkl', 'kendall.pkl']\n",
    "            # create empty list for actually existing files\n",
    "            existing_files = []\n",
    "\n",
    "             # 1. Check which files exist\n",
    "            for file_name in filenames:\n",
    "                path = Path(file_name)\n",
    "                if path.is_file():\n",
    "                    existing_files.append(path) # update list if existent\n",
    "                else: # print non-existent file names\n",
    "                    print(f\"\\n{file_name} not existent. Loading existent ones...\", end=\"\")\n",
    "\n",
    "            # Dictionary to hold loaded DataFrames\n",
    "            dfs_dict = {}\n",
    "\n",
    "            # 2. Load existing .pkl files\n",
    "            for path in existing_files:\n",
    "                try:\n",
    "                    # load pickled files \n",
    "                    df = pd.read_pickle(path)\n",
    "                    # create df with correlation coefficient names and get rid of the suffix\n",
    "                    dfs_dict[path.stem] = df \n",
    "                    print(f\"\\nLoaded {path.name}:\")\n",
    "                    print(tabulate(df, tablefmt='rounded_grid'))\n",
    "\n",
    "                    # Extract 'feature' column (if needed)\n",
    "                    values = df['feature'].tolist()\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error loading {path.name}: {e}\")\n",
    "\n",
    "                \n",
    "                # 3. Subset the dataset by each class in the key\n",
    "                try:\n",
    "                    class_names = dataset[key].unique().tolist()\n",
    "                except KeyError as e:\n",
    "                    print(f\"'{key}' not found in dataset: {e}\")\n",
    "                \n",
    "                # 4. loop through subset and plot correlation matrix\n",
    "                for class_name in class_names:\n",
    "                    subset_df = dataset[dataset[key] == class_name]\n",
    "                    selected_columns = subset_df[values]\n",
    "                    try:\n",
    "                        prepare_and_plot(data=selected_columns, coefficient=path.stem)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error processing '{class_name}' for correlation coefficient '{corr}': {e}\")\n",
    "\n",
    "        # if correlation coefficent is provided\n",
    "        else:\n",
    "\n",
    "            # 1. get unique key names\n",
    "            class_names = dataset[key].unique().tolist()\n",
    "            class_names\n",
    "\n",
    "            # create empty dict for the dfs to be saved into\n",
    "            keys_df={}\n",
    "\n",
    "            # 2. create subset for each unique class name\n",
    "            for class_name in class_names:\n",
    "                df = dataset[dataset[key] == class_name]\n",
    "                keys_df[class_name] = df\n",
    "            \n",
    "            # 3. for the class name and associated data, get items and create correlation plot\n",
    "            for class_name, df in keys_df.items():\n",
    "\n",
    "                # 3. prepare datasets and plot correlation matrix \n",
    "                prepare_and_plot(data=dataset, coefficient=corr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
